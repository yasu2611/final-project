# -*- coding: utf-8 -*-
"""Churn_Modelling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/121Mov0846D-61p-4lkGB1LECMQafEmTj
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.impute import SimpleImputer

# Models
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import classification_report

# Evaluation
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# 1. Load Dataset

df = pd.read_csv("/content/drive/MyDrive/Ml/Churn_Modelling.csv")
df

# 2. Handle Missing Values

print("Dataset preview:")
#print(df.head())
display(df)

display(pd.DataFrame(df.dtypes, columns=['Data Type']))

plt.figure(figsize=(4,3)) # use to get size to figures
_df_2.groupby('Data Type').size().plot(
    kind='barh',
    color=sns.mpl_palette('Dark2')
)
plt.title("Count of Columns by Data Type")
plt.xlabel("Number of Columns")
plt.ylabel("Data Type")
plt.gca().spines[['top','right']].set_visible(False)
plt.show()

num_cols = df.select_dtypes(include=['int64','float64']).columns.tolist()
cat_cols = df.select_dtypes(include=['object']).columns.tolist()

print("\nNumeric columns:")
print(num_cols)

print("\nCategorical columns:")
print(cat_cols)

print("Missing values before cleaning:")
display(df.isnull().sum().to_frame())

df[num_cols] = SimpleImputer(strategy='mean').fit_transform(df[num_cols]) # Fill missing values in numeric columns withcolumn average
df[cat_cols] = SimpleImputer(strategy="most_frequent").fit_transform(df[cat_cols]) #Fill missing values in categorical columns with most common value

# 3. Encode Categorical

#df = pd.get_dummies(df, columns=['Geography', 'Gender'], drop_first=True)
print(df.columns)

# Already encoded — no action needed
pass



#df = df.drop(['Surname'], axis=1)
#df = pd.get_dummies(df, columns=['Geography', 'Gender'], drop_first=True)

# 4. Choose Target Column

if "Exited" in df.columns:
    target = "Exited"
else:
    raise ValueError("Target column 'Exited' not found in dataset")

#print("\033[91mTarget Selected:\033[0m", target)
print("Target Selected:", target)

X = df.drop(columns=[target])
y = df[target]

# 5. Feature Scaling

from sklearn.preprocessing import MinMaxScaler

'''MinMaxScaler transforms all features into the range 0 → 1
Improves performance for models like:
Logistic Regression
SVM
KNN
Neural Networks'''

# Identify numeric columns again
num_cols = df.select_dtypes(include=['int64','float64']).columns.tolist()

'''Looks at your dataframe df
Selects columns whose data types are:
int64
float64
Converts them into a Python list'''

# Remove target column if present

if target in num_cols:
    num_cols.remove(target)

display(pd.DataFrame(num_cols, columns=["Numeric Columns"]))

# Apply scaling

scaler = MinMaxScaler()
X[num_cols] = scaler.fit_transform(X[num_cols])

# 6. Train-Test Split

X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=42, stratify=y )

"""test_size=0.3  →  30% test, 70% train
random_state=42  →  same split every time (reproducible)
stratify=y  →  keeps class ratio same in train & test (avoids bias)"""

models = {
    "Logistic Regression": LogisticRegression(max_iter=1000, class_weight="balanced", random_state=42),
    "KNN": KNeighborsClassifier(n_neighbors=5),
    "Decision Tree": DecisionTreeClassifier(random_state=42, class_weight="balanced"),
    "Random Forest": RandomForestClassifier(n_estimators=200, random_state=42, class_weight="balanced_subsample"),
    "SVM (RBF)": SVC(kernel="rbf", probability=True, class_weight="balanced"),
    "Naive Bayes": GaussianNB()
}

results = {}

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    acc = round(accuracy_score(y_test, y_pred) * 100, 2)
    results[name] = acc

    print("\n----------------------------------------")
    print(f"\033[94mModel: {name}\033[0m")   # Blue color title
    print("Accuracy:", acc, "%\n")

    print("Confusion Matrix:")
    print(confusion_matrix(y_test, y_pred))

    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, zero_division=0))

